{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf39d2-b465-45fe-9bc1-0630c6820d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 23:07:45.779850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-11 23:07:46.833226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:07:46.833261: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-11 23:07:46.966667: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-11 23:07:49.139045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:07:49.139217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:07:49.139236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error,r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "look_back=5000 # ~Past 6 months data to train on\n",
    "look_ahead=24 # Predict next look_ahead values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cc81c-edd3-4a1c-a0cd-9d1b03469fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv('data.csv',index_col=0)\n",
    "\n",
    "# [Feature Engineering]\n",
    "data.loc[data['ont_ene']<0,'ont_ene']=0\n",
    "\n",
    "mean=data['ont_ene'].mean() # Clipping outliers using zscore analysis\n",
    "std=data['ont_ene'].std()\n",
    "\n",
    "upperlimit=mean + 3 * std\n",
    "data.loc[data['ont_ene']>upperlimit,'ont_ene']=upperlimit # Can try setting it None and fill forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d4e67-5098-459b-b095-25785724297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Train Test Split]\n",
    "X=data.dropna().iloc[-look_back:,:-1].values # Keeping all the non-null rows as training data\n",
    "y=data.dropna().iloc[-look_back:,-1].values\n",
    "test=data[data.ont_ene.isnull()].iloc[:look_ahead,:-1].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train=np.reshape(y_train, (-1,1))\n",
    "y_val=np.reshape(y_val, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecf5da-c55a-4f34-bd44-dbbe6185d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = MinMaxScaler(feature_range = (0, 1))\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_val = sc_x.transform(X_val)\n",
    "\n",
    "sc_y = MinMaxScaler(feature_range = (0, 1))\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "y_val = sc_y.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490cdc9-f600-4e3c-8648-667e441867e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 23:08:12.583944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 23:08:12.584582: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.584784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.584937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585056: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585177: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585298: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-11 23:08:12.585546: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-11 23:08:12.587537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss = 'mae', optimizer = 'adam', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e277bd8-d259-44f8-9d73-7444a29a4e78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0753 - mae: 0.0753 - val_loss: 0.0655 - val_mae: 0.0655\n",
      "Epoch 2/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0636 - mae: 0.0636 - val_loss: 0.0593 - val_mae: 0.0593\n",
      "Epoch 3/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0609 - mae: 0.0609 - val_loss: 0.0605 - val_mae: 0.0605\n",
      "Epoch 4/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0594 - mae: 0.0594 - val_loss: 0.0569 - val_mae: 0.0569\n",
      "Epoch 5/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0583 - mae: 0.0583 - val_loss: 0.0592 - val_mae: 0.0592\n",
      "Epoch 6/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0577 - mae: 0.0577 - val_loss: 0.0562 - val_mae: 0.0562\n",
      "Epoch 7/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0570 - mae: 0.0570 - val_loss: 0.0558 - val_mae: 0.0558\n",
      "Epoch 8/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0562 - mae: 0.0562 - val_loss: 0.0582 - val_mae: 0.0582\n",
      "Epoch 9/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.0564 - val_loss: 0.0572 - val_mae: 0.0572\n",
      "Epoch 10/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.0555 - val_loss: 0.0543 - val_mae: 0.0543\n",
      "Epoch 11/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.0555 - val_loss: 0.0548 - val_mae: 0.0548\n",
      "Epoch 12/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0544 - mae: 0.0544 - val_loss: 0.0555 - val_mae: 0.0555\n",
      "Epoch 13/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0541 - mae: 0.0541 - val_loss: 0.0543 - val_mae: 0.0543\n",
      "Epoch 14/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0540 - mae: 0.0540 - val_loss: 0.0532 - val_mae: 0.0532\n",
      "Epoch 15/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0540 - mae: 0.0540 - val_loss: 0.0530 - val_mae: 0.0530\n",
      "Epoch 16/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0534 - mae: 0.0534 - val_loss: 0.0528 - val_mae: 0.0528\n",
      "Epoch 17/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0531 - mae: 0.0531 - val_loss: 0.0539 - val_mae: 0.0539\n",
      "Epoch 18/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0531 - mae: 0.0531 - val_loss: 0.0531 - val_mae: 0.0531\n",
      "Epoch 19/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0526 - mae: 0.0526 - val_loss: 0.0527 - val_mae: 0.0527\n",
      "Epoch 20/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0520 - mae: 0.0520 - val_loss: 0.0538 - val_mae: 0.0538\n",
      "Epoch 21/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0522 - mae: 0.0522 - val_loss: 0.0522 - val_mae: 0.0522\n",
      "Epoch 22/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0518 - mae: 0.0518 - val_loss: 0.0529 - val_mae: 0.0529\n",
      "Epoch 23/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0517 - mae: 0.0517 - val_loss: 0.0527 - val_mae: 0.0527\n",
      "Epoch 24/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0515 - mae: 0.0515 - val_loss: 0.0555 - val_mae: 0.0555\n",
      "Epoch 25/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0509 - mae: 0.0509 - val_loss: 0.0524 - val_mae: 0.0524\n",
      "Epoch 26/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0509 - mae: 0.0509 - val_loss: 0.0519 - val_mae: 0.0519\n",
      "Epoch 27/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0508 - mae: 0.0508 - val_loss: 0.0547 - val_mae: 0.0547\n",
      "Epoch 28/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0509 - mae: 0.0509 - val_loss: 0.0521 - val_mae: 0.0521\n",
      "Epoch 29/200\n",
      "609/609 [==============================] - 3s 5ms/step - loss: 0.0504 - mae: 0.0504 - val_loss: 0.0524 - val_mae: 0.0524\n",
      "Epoch 30/200\n",
      "609/609 [==============================] - 3s 4ms/step - loss: 0.0499 - mae: 0.0499 - val_loss: 0.0516 - val_mae: 0.0516\n",
      "Epoch 31/200\n",
      "609/609 [==============================] - 3s 5ms/step - loss: 0.0501 - mae: 0.0501 - val_loss: 0.0520 - val_mae: 0.0520\n",
      "Epoch 32/200\n",
      "609/609 [==============================] - 3s 5ms/step - loss: 0.0495 - mae: 0.0495 - val_loss: 0.0523 - val_mae: 0.0523\n",
      "Epoch 33/200\n",
      "609/609 [==============================] - 2s 4ms/step - loss: 0.0494 - mae: 0.0494 - val_loss: 0.0514 - val_mae: 0.0514\n",
      "Epoch 34/200\n",
      "609/609 [==============================] - 3s 5ms/step - loss: 0.0494 - mae: 0.0494 - val_loss: 0.0534 - val_mae: 0.0534\n",
      "Epoch 35/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0493 - mae: 0.0493 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 36/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0490 - mae: 0.0490 - val_loss: 0.0509 - val_mae: 0.0509\n",
      "Epoch 37/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0489 - mae: 0.0489 - val_loss: 0.0523 - val_mae: 0.0523\n",
      "Epoch 38/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0488 - mae: 0.0488 - val_loss: 0.0512 - val_mae: 0.0512\n",
      "Epoch 39/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0488 - mae: 0.0488 - val_loss: 0.0511 - val_mae: 0.0511\n",
      "Epoch 40/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0487 - mae: 0.0487 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 41/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0483 - mae: 0.0483 - val_loss: 0.0504 - val_mae: 0.0504\n",
      "Epoch 42/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0482 - mae: 0.0482 - val_loss: 0.0521 - val_mae: 0.0521\n",
      "Epoch 43/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0480 - mae: 0.0480 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 44/200\n",
      "609/609 [==============================] - 2s 4ms/step - loss: 0.0480 - mae: 0.0480 - val_loss: 0.0514 - val_mae: 0.0514\n",
      "Epoch 45/200\n",
      "609/609 [==============================] - 2s 4ms/step - loss: 0.0476 - mae: 0.0476 - val_loss: 0.0503 - val_mae: 0.0503\n",
      "Epoch 46/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0475 - mae: 0.0475 - val_loss: 0.0515 - val_mae: 0.0515\n",
      "Epoch 47/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0477 - mae: 0.0477 - val_loss: 0.0509 - val_mae: 0.0509\n",
      "Epoch 48/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0473 - mae: 0.0473 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 49/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0474 - mae: 0.0474 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 50/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0470 - mae: 0.0470 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 51/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0470 - mae: 0.0470 - val_loss: 0.0528 - val_mae: 0.0528\n",
      "Epoch 52/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0466 - mae: 0.0466 - val_loss: 0.0520 - val_mae: 0.0520\n",
      "Epoch 53/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0464 - mae: 0.0464 - val_loss: 0.0501 - val_mae: 0.0501\n",
      "Epoch 54/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0467 - mae: 0.0467 - val_loss: 0.0497 - val_mae: 0.0497\n",
      "Epoch 55/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0467 - mae: 0.0467 - val_loss: 0.0503 - val_mae: 0.0503\n",
      "Epoch 56/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0463 - mae: 0.0463 - val_loss: 0.0504 - val_mae: 0.0504\n",
      "Epoch 57/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0458 - mae: 0.0458 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 58/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0460 - mae: 0.0460 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 59/200\n",
      "609/609 [==============================] - 3s 4ms/step - loss: 0.0458 - mae: 0.0458 - val_loss: 0.0504 - val_mae: 0.0504\n",
      "Epoch 60/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0456 - mae: 0.0456 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 61/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0455 - mae: 0.0455 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 62/200\n",
      "609/609 [==============================] - 2s 4ms/step - loss: 0.0454 - mae: 0.0454 - val_loss: 0.0497 - val_mae: 0.0497\n",
      "Epoch 63/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0452 - mae: 0.0452 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 64/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0451 - mae: 0.0451 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 65/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0453 - mae: 0.0453 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 66/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0453 - mae: 0.0453 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 67/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0452 - mae: 0.0452 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 68/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0448 - mae: 0.0448 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 69/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0448 - mae: 0.0448 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 70/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0444 - mae: 0.0444 - val_loss: 0.0506 - val_mae: 0.0506\n",
      "Epoch 71/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0444 - mae: 0.0444 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 72/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0445 - mae: 0.0445 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 73/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0443 - mae: 0.0443 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 74/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0441 - mae: 0.0441 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 75/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0440 - mae: 0.0440 - val_loss: 0.0487 - val_mae: 0.0487\n",
      "Epoch 76/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0440 - mae: 0.0440 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 77/200\n",
      "609/609 [==============================] - 1s 2ms/step - loss: 0.0440 - mae: 0.0440 - val_loss: 0.0503 - val_mae: 0.0503\n",
      "Epoch 78/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0436 - mae: 0.0436 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 79/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0437 - mae: 0.0437 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 80/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0437 - mae: 0.0437 - val_loss: 0.0487 - val_mae: 0.0487\n",
      "Epoch 81/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0435 - mae: 0.0435 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 82/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0437 - mae: 0.0437 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 83/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0433 - mae: 0.0433 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 84/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0435 - mae: 0.0435 - val_loss: 0.0506 - val_mae: 0.0506\n",
      "Epoch 85/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0435 - mae: 0.0435 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 86/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0430 - mae: 0.0430 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 87/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0431 - mae: 0.0431 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 88/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0426 - mae: 0.0426 - val_loss: 0.0501 - val_mae: 0.0501\n",
      "Epoch 89/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0431 - mae: 0.0431 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 90/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0428 - mae: 0.0428 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 91/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0429 - mae: 0.0429 - val_loss: 0.0483 - val_mae: 0.0483\n",
      "Epoch 92/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0425 - mae: 0.0425 - val_loss: 0.0514 - val_mae: 0.0514\n",
      "Epoch 93/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0428 - mae: 0.0428 - val_loss: 0.0484 - val_mae: 0.0484\n",
      "Epoch 94/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0425 - mae: 0.0425 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 95/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0429 - mae: 0.0429 - val_loss: 0.0481 - val_mae: 0.0481\n",
      "Epoch 96/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0425 - mae: 0.0425 - val_loss: 0.0484 - val_mae: 0.0484\n",
      "Epoch 97/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0424 - mae: 0.0424 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 98/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0423 - mae: 0.0423 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 99/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0422 - mae: 0.0422 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 100/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0420 - mae: 0.0420 - val_loss: 0.0505 - val_mae: 0.0505\n",
      "Epoch 101/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0421 - mae: 0.0421 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 102/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0421 - mae: 0.0421 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 103/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0417 - mae: 0.0417 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 104/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0419 - mae: 0.0419 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 105/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0420 - mae: 0.0420 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 106/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0418 - mae: 0.0418 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 107/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0417 - mae: 0.0417 - val_loss: 0.0513 - val_mae: 0.0513\n",
      "Epoch 108/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0417 - mae: 0.0417 - val_loss: 0.0478 - val_mae: 0.0478\n",
      "Epoch 109/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0421 - mae: 0.0421 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 110/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0419 - mae: 0.0419 - val_loss: 0.0480 - val_mae: 0.0480\n",
      "Epoch 111/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0413 - mae: 0.0413 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 112/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0416 - mae: 0.0416 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 113/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0415 - mae: 0.0415 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 114/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0411 - mae: 0.0411 - val_loss: 0.0505 - val_mae: 0.0505\n",
      "Epoch 115/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0413 - mae: 0.0413 - val_loss: 0.0497 - val_mae: 0.0497\n",
      "Epoch 116/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0410 - mae: 0.0410 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 117/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0412 - mae: 0.0412 - val_loss: 0.0487 - val_mae: 0.0487\n",
      "Epoch 118/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0411 - mae: 0.0411 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 119/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0501 - val_mae: 0.0501\n",
      "Epoch 120/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0414 - mae: 0.0414 - val_loss: 0.0487 - val_mae: 0.0487\n",
      "Epoch 121/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0408 - mae: 0.0408 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 122/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 123/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 124/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0408 - mae: 0.0408 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 125/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 126/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 127/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0408 - mae: 0.0408 - val_loss: 0.0513 - val_mae: 0.0513\n",
      "Epoch 128/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0406 - mae: 0.0406 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 129/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0405 - mae: 0.0405 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 130/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 131/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0404 - mae: 0.0404 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 132/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0403 - mae: 0.0403 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 133/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0403 - mae: 0.0403 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 134/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0404 - mae: 0.0404 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 135/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0406 - mae: 0.0406 - val_loss: 0.0503 - val_mae: 0.0503\n",
      "Epoch 136/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0401 - mae: 0.0401 - val_loss: 0.0480 - val_mae: 0.0480\n",
      "Epoch 137/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0404 - mae: 0.0404 - val_loss: 0.0479 - val_mae: 0.0479\n",
      "Epoch 138/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0402 - mae: 0.0402 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 139/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0401 - mae: 0.0401 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 140/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0402 - mae: 0.0402 - val_loss: 0.0484 - val_mae: 0.0484\n",
      "Epoch 141/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0397 - mae: 0.0397 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 142/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0399 - mae: 0.0399 - val_loss: 0.0481 - val_mae: 0.0481\n",
      "Epoch 143/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 144/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0505 - val_mae: 0.0505\n",
      "Epoch 145/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0399 - mae: 0.0399 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 146/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 147/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0396 - mae: 0.0396 - val_loss: 0.0497 - val_mae: 0.0497\n",
      "Epoch 148/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0399 - mae: 0.0399 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 149/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0394 - mae: 0.0394 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 150/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0396 - mae: 0.0396 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 151/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0396 - mae: 0.0396 - val_loss: 0.0499 - val_mae: 0.0499\n",
      "Epoch 152/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0397 - mae: 0.0397 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 153/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0395 - mae: 0.0395 - val_loss: 0.0507 - val_mae: 0.0507\n",
      "Epoch 154/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0396 - mae: 0.0396 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 155/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0397 - mae: 0.0397 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 156/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0393 - mae: 0.0393 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 157/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0396 - mae: 0.0396 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 158/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0394 - mae: 0.0394 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 159/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0392 - mae: 0.0392 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 160/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0393 - mae: 0.0393 - val_loss: 0.0487 - val_mae: 0.0487\n",
      "Epoch 161/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0392 - mae: 0.0392 - val_loss: 0.0497 - val_mae: 0.0497\n",
      "Epoch 162/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 163/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0391 - mae: 0.0391 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 164/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0393 - mae: 0.0393 - val_loss: 0.0478 - val_mae: 0.0478\n",
      "Epoch 165/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0390 - mae: 0.0390 - val_loss: 0.0502 - val_mae: 0.0502\n",
      "Epoch 166/200\n",
      "609/609 [==============================] - 2s 2ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0503 - val_mae: 0.0503\n",
      "Epoch 167/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0388 - mae: 0.0388 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 168/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0485 - val_mae: 0.0485\n",
      "Epoch 169/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0390 - mae: 0.0390 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 170/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0387 - mae: 0.0387 - val_loss: 0.0482 - val_mae: 0.0482\n",
      "Epoch 171/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0391 - mae: 0.0391 - val_loss: 0.0484 - val_mae: 0.0484\n",
      "Epoch 172/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0387 - mae: 0.0387 - val_loss: 0.0481 - val_mae: 0.0481\n",
      "Epoch 173/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0386 - mae: 0.0386 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 174/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0388 - mae: 0.0388 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 175/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0389 - mae: 0.0389 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 176/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0386 - mae: 0.0386 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 177/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0388 - mae: 0.0388 - val_loss: 0.0493 - val_mae: 0.0493\n",
      "Epoch 178/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0385 - mae: 0.0385 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 179/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0388 - mae: 0.0388 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 180/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0386 - mae: 0.0386 - val_loss: 0.0491 - val_mae: 0.0491\n",
      "Epoch 181/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0386 - mae: 0.0386 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 182/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0383 - mae: 0.0383 - val_loss: 0.0484 - val_mae: 0.0484\n",
      "Epoch 183/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0387 - mae: 0.0387 - val_loss: 0.0495 - val_mae: 0.0495\n",
      "Epoch 184/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0387 - mae: 0.0387 - val_loss: 0.0494 - val_mae: 0.0494\n",
      "Epoch 185/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0384 - mae: 0.0384 - val_loss: 0.0479 - val_mae: 0.0479\n",
      "Epoch 186/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0384 - mae: 0.0384 - val_loss: 0.0498 - val_mae: 0.0498\n",
      "Epoch 187/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0384 - mae: 0.0384 - val_loss: 0.0492 - val_mae: 0.0492\n",
      "Epoch 188/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0382 - mae: 0.0382 - val_loss: 0.0486 - val_mae: 0.0486\n",
      "Epoch 189/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0383 - mae: 0.0383 - val_loss: 0.0481 - val_mae: 0.0481\n",
      "Epoch 190/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0382 - mae: 0.0382 - val_loss: 0.0489 - val_mae: 0.0489\n",
      "Epoch 191/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0382 - mae: 0.0382 - val_loss: 0.0483 - val_mae: 0.0483\n",
      "Epoch 192/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0379 - mae: 0.0379 - val_loss: 0.0488 - val_mae: 0.0488\n",
      "Epoch 193/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0380 - mae: 0.0380 - val_loss: 0.0501 - val_mae: 0.0501\n",
      "Epoch 194/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0382 - mae: 0.0382 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 195/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0380 - mae: 0.0380 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 196/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0380 - mae: 0.0380 - val_loss: 0.0508 - val_mae: 0.0508\n",
      "Epoch 197/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0382 - mae: 0.0382 - val_loss: 0.0496 - val_mae: 0.0496\n",
      "Epoch 198/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0381 - mae: 0.0381 - val_loss: 0.0510 - val_mae: 0.0510\n",
      "Epoch 199/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0381 - mae: 0.0381 - val_loss: 0.0500 - val_mae: 0.0500\n",
      "Epoch 200/200\n",
      "609/609 [==============================] - 2s 3ms/step - loss: 0.0378 - mae: 0.0378 - val_loss: 0.0488 - val_mae: 0.0488\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, y_train, batch_size = 32, epochs = 200,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97032f1c-6f0c-43ef-99ed-ac30402a34b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test = sc_x.transform(test)\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bdca0-eb8d-4a73-961d-3b13ebf1fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=sc_y.inverse_transform(y_pred)\n",
    "y_true=sc_y.inverse_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0177f-ba04-4ebd-882e-06586e61e01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[59.17]</td>\n",
       "      <td>[47.98119]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101.75]</td>\n",
       "      <td>[46.725418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10.31]</td>\n",
       "      <td>[42.23819]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13.21]</td>\n",
       "      <td>[36.608658]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[12.27]</td>\n",
       "      <td>[56.218853]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[13.47]</td>\n",
       "      <td>[70.16918]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[70.65171]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[37.45]</td>\n",
       "      <td>[59.71214]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[39.22]</td>\n",
       "      <td>[37.169163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[36.112976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.49]</td>\n",
       "      <td>[31.93024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[12.29]</td>\n",
       "      <td>[34.026676]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[3.48]</td>\n",
       "      <td>[34.713535]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[61.54]</td>\n",
       "      <td>[42.662502]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[18.11]</td>\n",
       "      <td>[45.431698]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[6.03]</td>\n",
       "      <td>[51.631992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[36.27]</td>\n",
       "      <td>[55.3042]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[41.42]</td>\n",
       "      <td>[53.70874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[52.79]</td>\n",
       "      <td>[41.262173]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[43.816658]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[13.32]</td>\n",
       "      <td>[16.53249]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[15.42]</td>\n",
       "      <td>[18.130543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[18.09]</td>\n",
       "      <td>[6.802318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[14.39]</td>\n",
       "      <td>[-2334.7273]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual Values Predicted Values\n",
       "0        [59.17]       [47.98119]\n",
       "1       [101.75]      [46.725418]\n",
       "2        [10.31]       [42.23819]\n",
       "3        [13.21]      [36.608658]\n",
       "4        [12.27]      [56.218853]\n",
       "5        [13.47]       [70.16918]\n",
       "6          [0.0]       [70.65171]\n",
       "7        [37.45]       [59.71214]\n",
       "8        [39.22]      [37.169163]\n",
       "9          [0.0]      [36.112976]\n",
       "10        [0.49]       [31.93024]\n",
       "11       [12.29]      [34.026676]\n",
       "12        [3.48]      [34.713535]\n",
       "13       [61.54]      [42.662502]\n",
       "14       [18.11]      [45.431698]\n",
       "15        [6.03]      [51.631992]\n",
       "16       [36.27]        [55.3042]\n",
       "17       [41.42]       [53.70874]\n",
       "18       [52.79]      [41.262173]\n",
       "19         [0.0]      [43.816658]\n",
       "20       [13.32]       [16.53249]\n",
       "21       [15.42]      [18.130543]\n",
       "22       [18.09]       [6.802318]\n",
       "23       [14.39]     [-2334.7273]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(y_true[:24],y_preds[:24])), columns = ['Actual Values', 'Predicted Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb2316-04e0-4467-a120-40de238da05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.26970881859461"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_true[:24],y_preds[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece8537-6b05-46fe-a1d9-53d74a7c4e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-382.63229075455087"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true[:24],y_preds[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e2bba5-24fa-4567-ac05-ef2b3d24a87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[70.97]</td>\n",
       "      <td>[10.131966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[53.88]</td>\n",
       "      <td>[2.9973075]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[32.15]</td>\n",
       "      <td>[11.653866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12.11]</td>\n",
       "      <td>[49.043747]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[50.751644]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[43.17]</td>\n",
       "      <td>[55.553806]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[12.81]</td>\n",
       "      <td>[60.86021]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[60.46]</td>\n",
       "      <td>[52.29269]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[50.9]</td>\n",
       "      <td>[52.588547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[11.68]</td>\n",
       "      <td>[50.801884]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[41.11]</td>\n",
       "      <td>[44.684322]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[49.18]</td>\n",
       "      <td>[36.32362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[5.18]</td>\n",
       "      <td>[33.47593]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[29.69]</td>\n",
       "      <td>[26.60523]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[74.24]</td>\n",
       "      <td>[26.510311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[68.85]</td>\n",
       "      <td>[39.748272]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[5.91]</td>\n",
       "      <td>[39.930008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[68.08]</td>\n",
       "      <td>[38.467815]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[28.59]</td>\n",
       "      <td>[32.791634]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[24.399204]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[17.89561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[104.26]</td>\n",
       "      <td>[8.763037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[82.57]</td>\n",
       "      <td>[18.801043]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[47.86]</td>\n",
       "      <td>[-4336.3667]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual Values Predicted Values\n",
       "0        [70.97]      [10.131966]\n",
       "1        [53.88]      [2.9973075]\n",
       "2        [32.15]      [11.653866]\n",
       "3        [12.11]      [49.043747]\n",
       "4          [0.0]      [50.751644]\n",
       "5        [43.17]      [55.553806]\n",
       "6        [12.81]       [60.86021]\n",
       "7        [60.46]       [52.29269]\n",
       "8         [50.9]      [52.588547]\n",
       "9        [11.68]      [50.801884]\n",
       "10       [41.11]      [44.684322]\n",
       "11       [49.18]       [36.32362]\n",
       "12        [5.18]       [33.47593]\n",
       "13       [29.69]       [26.60523]\n",
       "14       [74.24]      [26.510311]\n",
       "15       [68.85]      [39.748272]\n",
       "16        [5.91]      [39.930008]\n",
       "17       [68.08]      [38.467815]\n",
       "18       [28.59]      [32.791634]\n",
       "19         [0.0]      [24.399204]\n",
       "20         [0.0]       [17.89561]\n",
       "21      [104.26]       [8.763037]\n",
       "22       [82.57]      [18.801043]\n",
       "23       [47.86]     [-4336.3667]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(y_true[:24],y_preds[:24])), columns = ['Actual Values', 'Predicted Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049e689-6acb-4fa4-8fc6-aa521583d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.array([14.35,14.36,11.64,7.09,41.7,61.4,56.92]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ed3e3-7409-4948-97c9-98a89af6f82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.35],\n",
       "       [14.36],\n",
       "       [11.64],\n",
       "       [ 7.09],\n",
       "       [41.7 ],\n",
       "       [61.4 ],\n",
       "       [56.92]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab8cbf-4c00-4b7d-91e4-46ac600e5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.304742954799103"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_true[:7],y_preds[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87662e12-f44e-4538-bceb-20ee15ab6dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.867150967817736"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263d15c-d48e-4319-9d49-78ad7b3d938e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
